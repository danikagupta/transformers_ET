{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create model\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead),\n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src.permute(1, 0, 2))\n",
    "        output = self.decoder(tgt.permute(1, 0, 2), memory)\n",
    "        return self.final_layer(output).permute(1, 0, 2)\n",
    "    \n",
    "    def predict(self, src, steps_ahead=1):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            src = src.permute(1, 0, 2)  # Adjust dimensions\n",
    "            memory = self.encoder(src)  # Encode source sequence\n",
    "            tgt = torch.zeros_like(src)  # Initialize target sequence with zeros\n",
    "            for i in range(steps_ahead):\n",
    "                output = self.decoder(tgt, memory)  # Decode to get the next step\n",
    "                tgt = torch.cat((tgt[1:], output[-1:]), dim=0)  # Append output to target sequence, drop the first time step\n",
    "            predictions = self.final_layer(output)  # Apply final linear layer\n",
    "        return predictions.permute(1, 0, 2)  # Adjust dimensions to match input\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 1\n",
    "nhead = 1\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "input_dim = 1  # Because our input data has only one feature\n",
    "\n",
    "model = TimeSeriesTransformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 10, 1])\n",
      "torch.Size([52, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Generate training data\n",
    "#\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Data from Berkeley \n",
    "\n",
    "data=[0.511811024,\n",
    "0.94488189,1.771653543,2.322834646,2.834645669,3.74015748,4.05511811,3.149606299,2.047244094,1.929133858,0.62992126,0.31496063,0.62992126,0.905511811,1.417322835,2.480314961,\n",
    "3.11023622,2.952755906,4.291338583,3.11023622,2.165354331,1.181102362,0.708661417,0.354330709,0.354330709,0.31496063,1.338582677,2.125984252,3.11023622,3.661417323,\n",
    "4.251968504,3.385826772,1.417322835,1.417322835,0.866141732,0.275590551,0.393700787,0.94488189,0.669291339,1.968503937,3.228346457,3.858267717,3.818897638,\n",
    "3.897637795,2.401574803,2.047244094,0.826771654,0.511811024,0.905511811,1.181102362,1.181102362,3.149606299,3.11023622,4.05511811,3.976377953,3.700787402,2.401574803,1.535433071,\n",
    "0.472440945,0.31496063,0.905511811,1.023622047,1.692913386,2.795275591,2.913385827,3.267716535,3.937007874,3.74015748,2.677165354,1.614173228,0.62992126]\n",
    "\n",
    "# Prepare training data\n",
    "input_sequence_length = 10\n",
    "output_sequence_length = 10\n",
    "\n",
    "X = [data[i:i+input_sequence_length] for i in range(len(data)-input_sequence_length-output_sequence_length+1)]\n",
    "y = [data[i+input_sequence_length:i+input_sequence_length+output_sequence_length] for i in range(len(data)-input_sequence_length-output_sequence_length+1)]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)  # Add dimension for features\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 5.222888946533203\n",
      "Epoch 1/100, Loss: 5.218656063079834\n",
      "Epoch 2/100, Loss: 5.214419841766357\n",
      "Epoch 3/100, Loss: 5.210179805755615\n",
      "Epoch 4/100, Loss: 5.205934524536133\n",
      "Epoch 5/100, Loss: 5.201685428619385\n",
      "Epoch 6/100, Loss: 5.1974310874938965\n",
      "Epoch 7/100, Loss: 5.193171501159668\n",
      "Epoch 8/100, Loss: 5.188907623291016\n",
      "Epoch 9/100, Loss: 5.184637546539307\n",
      "Epoch 10/100, Loss: 5.180361747741699\n",
      "Epoch 11/100, Loss: 5.176080226898193\n",
      "Epoch 12/100, Loss: 5.171792507171631\n",
      "Epoch 13/100, Loss: 5.167500019073486\n",
      "Epoch 14/100, Loss: 5.163200378417969\n",
      "Epoch 15/100, Loss: 5.158894062042236\n",
      "Epoch 16/100, Loss: 5.154580593109131\n",
      "Epoch 17/100, Loss: 5.150261402130127\n",
      "Epoch 18/100, Loss: 5.145934104919434\n",
      "Epoch 19/100, Loss: 5.141600608825684\n",
      "Epoch 20/100, Loss: 5.137259483337402\n",
      "Epoch 21/100, Loss: 5.13291072845459\n",
      "Epoch 22/100, Loss: 5.128553867340088\n",
      "Epoch 23/100, Loss: 5.124190807342529\n",
      "Epoch 24/100, Loss: 5.119819164276123\n",
      "Epoch 25/100, Loss: 5.115438938140869\n",
      "Epoch 26/100, Loss: 5.111050605773926\n",
      "Epoch 27/100, Loss: 5.106655120849609\n",
      "Epoch 28/100, Loss: 5.102251052856445\n",
      "Epoch 29/100, Loss: 5.097836494445801\n",
      "Epoch 30/100, Loss: 5.093416690826416\n",
      "Epoch 31/100, Loss: 5.08898401260376\n",
      "Epoch 32/100, Loss: 5.084546089172363\n",
      "Epoch 33/100, Loss: 5.080097198486328\n",
      "Epoch 34/100, Loss: 5.075639724731445\n",
      "Epoch 35/100, Loss: 5.07117223739624\n",
      "Epoch 36/100, Loss: 5.066697120666504\n",
      "Epoch 37/100, Loss: 5.062215328216553\n",
      "Epoch 38/100, Loss: 5.057717800140381\n",
      "Epoch 39/100, Loss: 5.0532121658325195\n",
      "Epoch 40/100, Loss: 5.048701763153076\n",
      "Epoch 41/100, Loss: 5.0441741943359375\n",
      "Epoch 42/100, Loss: 5.039642333984375\n",
      "Epoch 43/100, Loss: 5.035100936889648\n",
      "Epoch 44/100, Loss: 5.030545234680176\n",
      "Epoch 45/100, Loss: 5.025979995727539\n",
      "Epoch 46/100, Loss: 5.021408557891846\n",
      "Epoch 47/100, Loss: 5.01682710647583\n",
      "Epoch 48/100, Loss: 5.012233734130859\n",
      "Epoch 49/100, Loss: 5.007627487182617\n",
      "Epoch 50/100, Loss: 5.003010272979736\n",
      "Epoch 51/100, Loss: 4.998387813568115\n",
      "Epoch 52/100, Loss: 4.993754863739014\n",
      "Epoch 53/100, Loss: 4.98910665512085\n",
      "Epoch 54/100, Loss: 4.984451770782471\n",
      "Epoch 55/100, Loss: 4.9797868728637695\n",
      "Epoch 56/100, Loss: 4.9751057624816895\n",
      "Epoch 57/100, Loss: 4.970417499542236\n",
      "Epoch 58/100, Loss: 4.965723991394043\n",
      "Epoch 59/100, Loss: 4.961016654968262\n",
      "Epoch 60/100, Loss: 4.956295967102051\n",
      "Epoch 61/100, Loss: 4.951564788818359\n",
      "Epoch 62/100, Loss: 4.946828365325928\n",
      "Epoch 63/100, Loss: 4.942081451416016\n",
      "Epoch 64/100, Loss: 4.937314987182617\n",
      "Epoch 65/100, Loss: 4.932541370391846\n",
      "Epoch 66/100, Loss: 4.927759170532227\n",
      "Epoch 67/100, Loss: 4.922963619232178\n",
      "Epoch 68/100, Loss: 4.918155193328857\n",
      "Epoch 69/100, Loss: 4.913338661193848\n",
      "Epoch 70/100, Loss: 4.9085164070129395\n",
      "Epoch 71/100, Loss: 4.903672218322754\n",
      "Epoch 72/100, Loss: 4.898826599121094\n",
      "Epoch 73/100, Loss: 4.893964767456055\n",
      "Epoch 74/100, Loss: 4.889095783233643\n",
      "Epoch 75/100, Loss: 4.884214878082275\n",
      "Epoch 76/100, Loss: 4.879321575164795\n",
      "Epoch 77/100, Loss: 4.874416828155518\n",
      "Epoch 78/100, Loss: 4.869502544403076\n",
      "Epoch 79/100, Loss: 4.8645782470703125\n",
      "Epoch 80/100, Loss: 4.859640121459961\n",
      "Epoch 81/100, Loss: 4.8546953201293945\n",
      "Epoch 82/100, Loss: 4.8497395515441895\n",
      "Epoch 83/100, Loss: 4.844768524169922\n",
      "Epoch 84/100, Loss: 4.839791297912598\n",
      "Epoch 85/100, Loss: 4.834801197052002\n",
      "Epoch 86/100, Loss: 4.829797744750977\n",
      "Epoch 87/100, Loss: 4.8247904777526855\n",
      "Epoch 88/100, Loss: 4.819764137268066\n",
      "Epoch 89/100, Loss: 4.8147292137146\n",
      "Epoch 90/100, Loss: 4.80968713760376\n",
      "Epoch 91/100, Loss: 4.804630279541016\n",
      "Epoch 92/100, Loss: 4.799567222595215\n",
      "Epoch 93/100, Loss: 4.794491767883301\n",
      "Epoch 94/100, Loss: 4.789401531219482\n",
      "Epoch 95/100, Loss: 4.784306049346924\n",
      "Epoch 96/100, Loss: 4.77919340133667\n",
      "Epoch 97/100, Loss: 4.774075984954834\n",
      "Epoch 98/100, Loss: 4.768947124481201\n",
      "Epoch 99/100, Loss: 4.763808250427246\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train model\n",
    "#\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# In the training loop\n",
    "output = model(X[:-1], y[1:])  # Exclude the last target value\n",
    "loss = criterion(output.squeeze(-1), y[1:].squeeze(-1))  # Exclude the first target value\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X[:-1], y[1:])  # Assuming y is the target tensor\n",
    "    loss = criterion(output.squeeze(-1), y[1:].squeeze(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new data: torch.Size([1, 10, 1])\n",
      "Predictions: tensor([[[-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978],\n",
      "         [-0.0978]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions\n",
    "# Assume `new_data` is a tensor containing your new or unseen data, with dimensions [Batch Size, Sequence Length, Feature Dimension]\n",
    "X_data= [data[i:i+input_sequence_length] for i in range(len(data)-input_sequence_length-output_sequence_length+1)]\n",
    "new_data = torch.tensor(X_data[-1:], dtype=torch.float32).unsqueeze(-1)  # For example, using the last sequence from X_data\n",
    "print(f\"Shape of new data: {new_data.shape}\")\n",
    "steps_ahead = 10  # Assume you want to forecast 10 steps ahead\n",
    "predictions = model.predict(new_data, steps_ahead)\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
